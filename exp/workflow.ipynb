{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\a\\envs\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\a\\envs\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "# from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from typing import Literal, List\n",
    "from typing_extensions import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import json\n",
    "# from IPython.display import Image\n",
    "import easyocr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(api_key = GOOGLE_API_KEY, model = \"gemini-1.5-pro\",temperature = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I can summarize medical reports.  However, I am an AI and **cannot provide medical advice**.  My summaries are for informational purposes only and should not be used to make decisions about your health.  Always consult with a qualified medical professional for diagnosis and treatment.\n",
      "\n",
      "I can help by:\n",
      "\n",
      "* **Extracting key findings:**  I can identify and present the most important results from lab tests, imaging studies, and other diagnostic procedures.\n",
      "* **Summarizing diagnoses and treatment plans:** I can condense the information about the diagnosed conditions and the recommended course of action.\n",
      "* **Simplifying complex medical jargon:** I can explain technical terms in more accessible language.\n",
      "* **Identifying medications and dosages:** I can list the prescribed medications, their dosages, and frequency of administration.\n",
      "* **Highlighting follow-up recommendations:** I can point out scheduled appointments and recommended next steps.\n",
      "\n",
      "To help me provide the best summary, please provide me with the full text of the report or relevant sections.  Be sure to redact any personally identifiable information (PII) like your name, address, and medical record number before sharing the report.\n",
      "\n",
      "While I strive to be accurate, it's crucial to remember that my interpretations are based on pattern recognition and may not always be perfect.  **Always verify my summary with your doctor.**  They can provide context, answer your questions, and ensure the information is understood correctly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "R = llm.invoke(f\"Can you summarize medical reports?\").content\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    report : str\n",
    "    generation : str    \n",
    "    summary : str\n",
    "    translation: str\n",
    "    anamoly : str\n",
    "    root_cause : str\n",
    "    path : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr(state):\n",
    "    path = state[\"path\"]\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    result = reader.readtext(path)\n",
    "    response = [detection[1] for detection in result]\n",
    "    response_text = '\\n'.join(response)\n",
    "    print(\"HI\")\n",
    "    return {\"report\" : response_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(state):\n",
    "    report = state[\"report\"]\n",
    "    response = llm.invoke(f\"You are being provided a medical report correct this grammatically{report} Note: Do not add anything extra. Just return the report as it is.\").content\n",
    "    print(\"INSIDE REPORT\")\n",
    "    return {\"generation\" : response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(state):\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    response = llm.invoke(f\"\"\"You are an Expert in Evaluating medical Reports.You are given the report, \n",
    "                          Based on the report Devise a comprehensive of the report in not more than 150 words.\n",
    "                          Note: The summary you generate should be generated in layman terms. Keep the summary as \n",
    "                          simple as possible. report {generation}\"\"\").content\n",
    "    print(\"INSIDE SUMMARY\")\n",
    "    print(response)\n",
    "    \n",
    "    return {\"summary\" : response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anamoly_detection(state):\n",
    "    summary = state[\"summary\"]\n",
    "\n",
    "    class Route_Anamoly(BaseModel):\n",
    "        Binary_Score: str = Field(..., description=\"Does this report contain abnormal values? Yes or No\")\n",
    "\n",
    "    structured_llm = llm.with_structured_output(Route_Anamoly)\n",
    "\n",
    "    system = \"\"\"\n",
    "    You are provided with a summary of a medical report containing various measurements and observations. \n",
    "    Your task is to identify any abnormal values or measurements. A value is considered abnormal if it is described as \"elevated,\" \"low,\" \"absent,\" or if it falls outside the reference range.\n",
    "    If any abnormal values are found, respond with 'Yes'. If no abnormal values are found, or if the summary lacks numerical data, respond with 'No'.\n",
    "    \"\"\"\n",
    "\n",
    "    binary_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"report:{report}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    grader_chain = binary_prompt | structured_llm\n",
    "\n",
    "    llm_response = grader_chain.invoke({\"report\": summary})\n",
    "    print(\"Detecting anamoly\")\n",
    "    if llm_response.Binary_Score == \"Yes\":\n",
    "        return \"Anamoly\"\n",
    "    else:\n",
    "        return \"Normal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_extractor(state):\n",
    "    summary = state[\"summary\"]\n",
    "\n",
    "    system = \"\"\"\n",
    "    You are given a medical report summary that includes various measurements and observations. \n",
    "    Your task is to identify and extract all values or measurements that are described as abnormal. \n",
    "    A value is considered abnormal if it is described as \"elevated,\" \"low,\" \"absent,\" or if it is outside the reference range.\n",
    "    Return the abnormal values along with the associated measurement.\n",
    "    If no abnormal values are found, respond with 'None'.\n",
    "    \"\"\"\n",
    "    print(\"hey\")\n",
    "    extraction_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", f\"summary: {summary}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = extraction_prompt | llm\n",
    "    extracted_values = chain.invoke({summary : summary})\n",
    "    extracted_values = extracted_values.content\n",
    "    print(\"Exiting the extacted values node\")\n",
    "    print(extracted_values)\n",
    "    return {\"anamoly\": extracted_values}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_cause(state):\n",
    "    summary = state[\"summary\"]\n",
    "\n",
    "    system = \"\"\"\n",
    "            Hey, You are given a summary of the report. Based on the summary you have to generate\n",
    "            The possible Root causes of the summary and provide it in a concise manner\"\"\"\n",
    "    \n",
    "    extraction_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            \"human\", f\"summary : {summary}\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = extraction_prompt | llm\n",
    "    root_causes = chain.invoke({summary : summary})\n",
    "    root_causes = root_causes.content\n",
    "    print(root_causes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_cause(state):#THIS IS A NODE\n",
    "    anamoly = state[\"anamoly\"]\n",
    "    response = llm.invoke(f\"\"\"From the given extracted values find out the root causes.\n",
    "                          NOTE: Only find out root cause and nothing else. Values{anamoly}\"\"\")\n",
    "    response = response.content\n",
    "\n",
    "    print(response)\n",
    "    return {\"root_cause\" : response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_cause_1(state):\n",
    "    anamoly = state[\"anamoly\"]\n",
    "\n",
    "    response_1 = llm.invoke(f\"\"\"From the given extracted values find out the root causes.\n",
    "                            NOTE: Only find out root cause and nothing else. Values{anamoly}\"\"\")\n",
    "    response_1 = response_1.content\n",
    "\n",
    "    print(response_1)\n",
    "    return {\"root_cause\" : response_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"ocr_node\", ocr)\n",
    "workflow.add_node(\"report_node\", report)\n",
    "workflow.add_node(\"generate_summary_node\", generate_summary)\n",
    "# workflow.add_node(\"Translation_node\",Translate_Summary)\n",
    "workflow.add_node(\"value_extractor_node\", value_extractor)\n",
    "workflow.add_node(\"root_cause_node\", root_cause)\n",
    "workflow.add_node(\"root_cause_1_node\", root_cause_1)\n",
    "\n",
    "workflow.add_edge(START, \"ocr_node\")\n",
    "workflow.add_edge(\"ocr_node\", \"report_node\")\n",
    "workflow.add_edge(\"report_node\", \"generate_summary_node\")\n",
    "# workflow.add_edge(\"generate_summary_node\",\"Translation_node\")\n",
    "# workflow.add_edge(\"Translation_node\",END)\n",
    "workflow.add_edge(\"generate_summary_node\", END)\n",
    "\n",
    "workflow.add_conditional_edges(\"generate_summary_node\", anamoly_detection,{\n",
    "    \"Anamoly\" : \"value_extractor_node\",\n",
    "    \"Normal\": \"root_cause_1_node\"\n",
    "})\n",
    "\n",
    "workflow.add_edge(\"root_cause_1_node\", END)\n",
    "\n",
    "workflow.add_edge(\"value_extractor_node\", \"root_cause_node\")\n",
    "workflow.add_edge(\"root_cause_node\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
